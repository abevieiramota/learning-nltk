{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "# sent_tokenize uses an instance of PunktSentenceTokenizer \n",
    "#    from the nltk.tokenize.punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para = \"Hello World. É bom ver você. Obrigado por comprar este livro, Mr. Abelardo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.',\n",
       " 'É bom ver você.',\n",
       " 'Obrigado por comprar este livro, Mr. Abelardo.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(para)\n",
    "# linguagem default = english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cada chamada a sent_tokenize carrega um tokenizer -> para processar várias vezes, melhor instanciar um tokenizer e reutilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.',\n",
       " 'É bom ver você.',\n",
       " 'Obrigado por comprar este livro, Mr. Abelardo.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/portuguese.pickle\")\n",
    "\n",
    "tokenizer.tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"Hello World.\")\n",
    "# linguagem default = english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "treebank_tokenizer.tokenize(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca', \"n't\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_tokenizer.tokenize(\"can't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PunktWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removido na versão 3 > ver: https://github.com/nltk/nltk/pull/746"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can', \"'\", 't']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "word_punct_tokenizer.tokenize(\"can't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing sentences using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"can't\", 'do', 'anything']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "regexp_tokenize(\"can't do anything\", \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Não', 'posso', 'mais', 'te', 'ver', 'Não', 'insista']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "regexp_tokenizer_1 = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "regexp_tokenizer_1.tokenize(\"Não posso mais te ver! Não insista!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Não', 'posso', 'mais', 'te', 'ver!', 'Não', 'insista!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenizer_2 = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "\n",
    "regexp_tokenizer_2.tokenize(\"Não posso mais te ver! Não insista!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"can't do anything\", 'anything']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "\n",
    "blankline_tokenizer = BlanklineTokenizer()\n",
    "\n",
    "blankline_tokenizer.tokenize(\"can't do anything \\n\\n anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering stopwords in a tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Não', 'posso', 'ver', 'Não', 'insista']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "pt_stops = set(stopwords.words(\"portuguese\"))\n",
    "# stopwords.words() retorna uma lista de stopwords de todas as linguagens disponíveis\n",
    "\n",
    "words = regexp_tokenizer_1.tokenize(\"Não posso mais te ver! Não insista!\")\n",
    "\n",
    "[word for word in words if word not in pt_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203,\n",
       " ['pela',\n",
       "  'esteja',\n",
       "  'tenho',\n",
       "  'são',\n",
       "  'depois',\n",
       "  'tínhamos',\n",
       "  'houveriam',\n",
       "  'da',\n",
       "  'hajamos',\n",
       "  'suas'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pt_stops), list(pt_stops)[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linguagens disponíveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking up synsets for a word in WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cookbook.n.01', 'a book of recipes and cooking directions')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets(\"cookbook\")[0]\n",
    "\n",
    "syn.name(), syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('cookbook.n.01')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset(syn.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they had sex in the back seat']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"sex\")[0].examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bodily_process.n.01')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# menos específicos\n",
    "wordnet.synsets(\"sex\")[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('autoeroticism.n.01'),\n",
       " Synset('bestiality.n.02'),\n",
       " Synset('bisexuality.n.02'),\n",
       " Synset('bondage.n.03'),\n",
       " Synset('carnal_abuse.n.01'),\n",
       " Synset('conception.n.02'),\n",
       " Synset('coupling.n.03'),\n",
       " Synset('foreplay.n.01'),\n",
       " Synset('heterosexuality.n.01'),\n",
       " Synset('homosexuality.n.01'),\n",
       " Synset('lechery.n.01'),\n",
       " Synset('outercourse.n.01'),\n",
       " Synset('perversion.n.02'),\n",
       " Synset('pleasure.n.05'),\n",
       " Synset('promiscuity.n.01'),\n",
       " Synset('reproduction.n.05'),\n",
       " Synset('safe_sex.n.01'),\n",
       " Synset('sexual_intercourse.n.01'),\n",
       " Synset('sexual_love.n.02')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mais específicos\n",
    "wordnet.synsets(\"sex\")[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"sex\")[0].root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('make.v.03')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset(\"can.v.01\").root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('process.n.06'),\n",
       "  Synset('organic_process.n.01'),\n",
       "  Synset('bodily_process.n.01'),\n",
       "  Synset('sexual_activity.n.01')]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"sex\")[0].hypernym_paths()\n",
    "# ?por que é list de list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech(POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four common POS found in WordNet.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Part-of-speech</th>\n",
    "        <th>Tag</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Noun</td>\n",
    "        <td>n</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Adjective</td>\n",
    "        <td>a</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Adverb</td>\n",
    "        <td>r</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Verb</td>\n",
    "        <td>v</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'n': [Synset('great.n.01')],\n",
       "             's': [Synset('great.s.01'),\n",
       "              Synset('great.s.02'),\n",
       "              Synset('great.s.03'),\n",
       "              Synset('bang-up.s.01'),\n",
       "              Synset('capital.s.03'),\n",
       "              Synset('big.s.13')]})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "def redutor(d, s):\n",
    "    \n",
    "    d[s.pos()].append(s)\n",
    "    \n",
    "    return d\n",
    "\n",
    "reduce(redutor, wordnet.synsets(\"great\"), defaultdict(list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking up lemmas and synonyms in WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemas(word, syn=False):\n",
    "    \n",
    "    print(\"synsets:\\n\")\n",
    "    if syn:\n",
    "        \n",
    "        synsets = [wordnet.synset(word)]\n",
    "    else:\n",
    "        \n",
    "        synsets = wordnet.synsets(word)\n",
    "        \n",
    "    for syn in synsets:\n",
    "        \n",
    "        print(\"[{}]\".format(syn.name()))\n",
    "        print(\"lemmas:\")\n",
    "        for lem in syn.lemmas():\n",
    "            \n",
    "            print(\"\\t{}\".format(lem.name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets:\n",
      "\n",
      "[cookbook.n.01]\n",
      "lemmas:\n",
      "\tcookbook\n",
      "\tcookery_book\n"
     ]
    }
   ],
   "source": [
    "lemas(\"cookbook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets:\n",
      "\n",
      "[button.n.01]\n",
      "lemmas:\n",
      "\tbutton\n",
      "[push_button.n.01]\n",
      "lemmas:\n",
      "\tpush_button\n",
      "\tpush\n",
      "\tbutton\n",
      "[button.n.03]\n",
      "lemmas:\n",
      "\tbutton\n",
      "[button.n.04]\n",
      "lemmas:\n",
      "\tbutton\n",
      "[clitoris.n.01]\n",
      "lemmas:\n",
      "\tclitoris\n",
      "\tclit\n",
      "\tbutton\n",
      "[release.n.08]\n",
      "lemmas:\n",
      "\trelease\n",
      "\tbutton\n",
      "[button.n.07]\n",
      "lemmas:\n",
      "\tbutton\n",
      "[button.v.01]\n",
      "lemmas:\n",
      "\tbutton\n",
      "[button.v.02]\n",
      "lemmas:\n",
      "\tbutton\n"
     ]
    }
   ],
   "source": [
    "lemas(\"button\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### good 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moral excellence or admirableness'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = wordnet.synset(\"good.n.02\")\n",
    "\n",
    "good.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evil'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evil = good.lemmas()[0].antonyms()[0]\n",
    "\n",
    "evil.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quality of being morally wrong in principle or practice'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evil.synset().definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### good 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'having desirable or positive qualities especially those suitable for a thing specified'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = wordnet.synset(\"good.a.01\")\n",
    "\n",
    "good.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = good.lemmas()[0].antonyms()[0]\n",
    "\n",
    "bad.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'having undesirable or negative qualities'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad.synset().definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating WordNet synset similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*wup_similarity* is short for *Wu-Palmer Similarity*, which is a scoring method based on how similar the word senses are and where the synsets occur relative to each other in the hypernym tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9166666666666666, 0.9166666666666666)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb = wordnet.synset(\"cookbook.n.01\")\n",
    "ib = wordnet.synset(\"instruction_book.n.01\")\n",
    "\n",
    "cb.wup_similarity(ib), ib.wup_similarity(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Synset('reference_book.n.01')], [Synset('reference_book.n.01')])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.hypernyms(), ib.hypernyms()\n",
    "# same hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.wup_similarity(wordnet.synset(\"book.n.01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.shortest_path_distance(wordnet.synset(\"book.n.01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('work.n.02'),\n",
       " Synset('entity.n.01'),\n",
       " Synset('object.n.01'),\n",
       " Synset('book.n.01'),\n",
       " Synset('product.n.02'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('creation.n.02'),\n",
       " Synset('artifact.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('publication.n.01')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.common_hypernyms(wordnet.synset(\"book.n.01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38095238095238093"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.wup_similarity(wordnet.synset(\"dog.n.01\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cook = wordnet.synset(\"cook.v.01\")\n",
    "bake = wordnet.synset(\"bake.v.02\")\n",
    "\n",
    "cook.wup_similarity(bake)\n",
    "# pelo visto a similaridade mudou :M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path and LCH similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.path_similarity(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07142857142857142"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.path_similarity(wordnet.synset(\"dog.n.01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 500 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.538973871058276"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cb.lch_similarity(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985288301111273"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.lch_similarity(wordnet.synset(\"dog.n.01\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering word collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures\n",
    "from nltk.collocations import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [w.lower() for w in webtext.words(\"grail.txt\")]\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's remove punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 'knight'),\n",
       " ('clop', 'clop'),\n",
       " ('head', 'knight'),\n",
       " ('mumble', 'mumble')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def make_cfs(filename):\n",
    "    \n",
    "    words = []\n",
    "\n",
    "    with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "\n",
    "        text = ' '.join(f.readlines())\n",
    "        words = word_tokenize(text, language=\"portuguese\")\n",
    "        \n",
    "    stopset = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "    filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "\n",
    "    bcf = BigramCollocationFinder.from_words(words)\n",
    "    tcf = TrigramCollocationFinder.from_words(words)\n",
    "\n",
    "    bcf.apply_word_filter(filter_stops)\n",
    "    bcf.apply_freq_filter(3)\n",
    "    \n",
    "    tcf.apply_word_filter(filter_stops)\n",
    "    tcf.apply_freq_filter(3)\n",
    "    \n",
    "    return bcf, tcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alguma', 'coisa'),\n",
       " ('cada', 'vez'),\n",
       " ('Aos', 'poucos'),\n",
       " ('olhos', 'abertos'),\n",
       " ('Não', 'sei')]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clarice_bcf, clarice_tcf = make_cfs('perto_do_coracao_selvagem.txt')\n",
    "\n",
    "clarice_bcf.nbest(BigramAssocMeasures.likelihood_ratio, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('havia', 'alguma', 'coisa'),\n",
       " ('alguma', 'coisa', '...'),\n",
       " ('Não', 'sei', '...')]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clarice_tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('José', 'Dias'),\n",
       " ('prima', 'Justina'),\n",
       " ('tio', 'Cosme'),\n",
       " ('Minha', 'mãe'),\n",
       " ('Padre', 'Cabral')]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casmurro_bcf, casmurro_tcf = make_cfs('dom_casmurro.txt')\n",
    "\n",
    "casmurro_bcf.nbest(BigramAssocMeasures.likelihood_ratio, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('José', 'Dias', 'achou'),\n",
       " ('Sr.', 'José', 'Dias'),\n",
       " ('posso', 'ser', 'padre'),\n",
       " ('veio', 'ter', 'comigo')]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casmurro_tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Direito', 'Público'),\n",
       " ('atenção', 'voltada'),\n",
       " ('segunda', 'vertigem'),\n",
       " ('prima', 'Isabel'),\n",
       " ('profundezas', 'chamo'),\n",
       " ('profúndezas', 'chamo'),\n",
       " ('Papai', 'morreu'),\n",
       " ('Estou', 'sofrendo')]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filtra(juntas, pt1_pt2, N): \n",
    "    \n",
    "    return -1*(sum(pt1_pt2) - juntas)\n",
    "\n",
    "list(clarice_bcf.above_score(filtra, -15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Direito', 'Público'), -4.0),\n",
       " (('atenção', 'voltada'), -8.0),\n",
       " (('segunda', 'vertigem'), -9.0),\n",
       " (('prima', 'Isabel'), -11.0),\n",
       " (('profundezas', 'chamo'), -12.0),\n",
       " (('profúndezas', 'chamo'), -12.0),\n",
       " (('Papai', 'morreu'), -13.0),\n",
       " (('Estou', 'sofrendo'), -14.0),\n",
       " (('primeiro', 'romance'), -17.0),\n",
       " (('meia', 'escuridão'), -18.0)]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(clarice_bcf.score_ngrams(filtra))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Passeio', 'Público'), -4.0),\n",
       " (('cigana', 'oblíqua'), -5.0),\n",
       " (('Dom', 'Casmurro'), -7.0),\n",
       " (('santos', 'óleos'), -7.0),\n",
       " (('Engenho', 'Novo'), -9.0),\n",
       " (('Nosso', 'Senhor'), -10.0),\n",
       " (('Santa', 'Mônica'), -11.0),\n",
       " (('arte', 'fina'), -13.0),\n",
       " (('juízo', 'final'), -13.0),\n",
       " (('serás', 'feliz'), -13.0)]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(casmurro_bcf.score_ngrams(filtra))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
